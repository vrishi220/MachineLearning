{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLhw1prob5",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrishi220/MachineLearning/blob/master/MLhw1prob52.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9pPaS7j2KYky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "rate, batch_size, classes_size, epochs, input_size, final  = 0.05, 100, 10, 50, 28**2, []\n",
        "\n",
        "# batch_size = 128\n",
        "# num_classes = 10\n",
        "# epochs = 30\n",
        "\n",
        "# Reshape the training and testing data of a single dimension by 28*28. \n",
        "# Then the values of the reshaped vector are taken as float32 values for the ease of readability.\n",
        "# Then the values are divided by 255 (max count for colors) so as to make all values withiin 0 to 1.\n",
        "def reshapeAndReduce(X): return X.reshape(X.shape[0],X.shape[1]*X.shape[2]).astype('float32')/255\n",
        "\n",
        "# Convert vector into an Numpy array of one-hot-encodings which are float values\n",
        "def getOneHot(X,classes_size):\n",
        "  one_hot_vec = []\n",
        "  for i in X: one_hot_vec.append([1 if x == i else 0 for x in range(classes_size)])\n",
        "  return np.array(one_hot_vec).astype('float32')\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "var_dict = {'con':[[],[]],\n",
        "            'x_width':[[],[]],\n",
        "            'x_height':[[],[]],\n",
        "            'x_bw':[[[],[]],[[],[]]],\n",
        "            'x_row':[np.sum(x_train, axis = 1, keepdims = True),np.sum(x_test, axis = 1, keepdims = True)],\n",
        "            'x_col':[np.sum(x_train, axis = 2, keepdims = True),np.sum(x_test, axis = 2, keepdims = True)]}\n",
        "\n",
        "a = 0\n",
        "# TODO\n",
        "for each in ((x_train, y_train),(x_test, y_test)):\n",
        "  for i in range(each[1].shape[0]):\n",
        "    \n",
        "    c1,c2,c3 = 0,0,0\n",
        "    \n",
        "    if each[1][i] == 8: var_dict['con'][a].append(1)\n",
        "    elif each[1][i] in [0,6,9]: var_dict['con'][a].append(2/3)\n",
        "    else: var_dict['con'][a].append(1/3)\n",
        "      \n",
        "    for j in range(28):\n",
        "      if var_dict['x_row'][a][i][0][j] > 0: c1 += 1\n",
        "      if var_dict['x_col'][a][i][j][0] > 0: c2 += 1\n",
        "      for k in range(28):\n",
        "        if each[0][i][j][k] == 0: c3 += 1\n",
        "          \n",
        "    var_dict['x_width'][a].append(c1/28)\n",
        "    var_dict['x_height'][a].append(c2/28)\n",
        "    var_dict['x_bw'][a][0].append(c3/784)\n",
        "    var_dict['x_bw'][a][1].append(1-(c3/784))\n",
        "    \n",
        "  a+=1\n",
        "\n",
        "\n",
        "# # connected components for training and testing images      \n",
        "# con_comp_train = list()\n",
        "# con_comp_test = list()\n",
        "# for i in range(60000):\n",
        "#   if y_train[i] == 1 or y_train[i] == 2 or y_train[i] == 3 or y_train[i] == 4 or y_train[i] == 5 or y_train[i] == 7:\n",
        "#     con_comp_train.append(1/3)\n",
        "#   if y_train[i] == 0 or y_train[i] == 6 or y_train[i] == 9:\n",
        "#     con_comp_train.append(2/3)\n",
        "#   if y_train[i] == 8:\n",
        "#     con_comp_train.append(1)\n",
        "    \n",
        "# for i in range(10000):\n",
        "#   if y_test[i] == 1 or y_test[i] == 2 or y_test[i] == 3 or y_test[i] == 4 or y_test[i] == 5 or y_test[i] == 7:\n",
        "#     con_comp_test.append(1/3)\n",
        "#   if y_test[i] == 0 or y_test[i] == 6 or y_test[i] == 9:\n",
        "#     con_comp_test.append(2/3)\n",
        "#   if y_test[i] == 8:\n",
        "#     con_comp_test.append(1)\n",
        "\n",
        "# # width of training and testing images\n",
        "# x_train_width = list()\n",
        "# for i in range(60000):\n",
        "#   count = 0\n",
        "#   for j in range(28):\n",
        "#     if x_train_row[i][0][j] > 0:\n",
        "#       count += 1\n",
        "#   x_train_width.append(count/28)\n",
        "# x_test_width = list()\n",
        "# for i in range(10000):\n",
        "#   count = 0\n",
        "#   for j in range(28):\n",
        "#     if x_test_row[i][0][j] > 0:\n",
        "#       count += 1\n",
        "#   x_test_width.append(count/28)\n",
        "\n",
        "# # height of training and testing images\n",
        "# x_train_col = np.sum(x_train, axis = 2, keepdims = True)\n",
        "# x_test_col = np.sum(x_test, axis = 2, keepdims = True)\n",
        "# x_train_height = list()\n",
        "# for i in range(60000):\n",
        "#   count = 0\n",
        "#   for j in range(28):\n",
        "#     if x_train_col[i][j][0] > 0:\n",
        "#       count += 1\n",
        "#   x_train_height.append(count/28)\n",
        "# x_test_height = list()\n",
        "# for i in range(10000):\n",
        "#   count = 0\n",
        "#   for j in range(28):\n",
        "#     if x_test_col[i][j][0] > 0:\n",
        "#       count += 1\n",
        "#   x_test_height.append(count/28)\n",
        "\n",
        "# # number of black and white spaces in training and testing images\n",
        "# x_train_black = list()\n",
        "# x_train_white = list()\n",
        "\n",
        "# for i in range(60000):\n",
        "#   count = 0\n",
        "#   for j in range(28):\n",
        "#     for k in range(28):\n",
        "#       if x_train[i][j][k] == 0:\n",
        "#         count += 1\n",
        "#   x_train_black.append(count/784)\n",
        "#   x_train_white.append(1-(count/784))\n",
        "# x_test_black = list()\n",
        "# x_test_white = list()\n",
        "# for i in range(10000):\n",
        "#   count = 0\n",
        "#   for j in range(28):\n",
        "#     for k in range(28):\n",
        "#       if x_test[i][j][k] == 0:\n",
        "#         count += 1\n",
        "#   x_test_black.append(count/784)\n",
        "#   x_test_white.append(1-(count/784))\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      # # Reshape the training and test data.\n",
        "# X_train, X_test = reshapeAndReduce(x_train), reshapeAndReduce(x_test)\n",
        "\n",
        "# # Convert class vector to binary class matrix (one-hot-encoding).\n",
        "# Y_train, Y_test = getOneHot(y_train, classes_size), getOneHot(y_test, classes_size)\n",
        "\n",
        "# # Initialize a sequential model\n",
        "# model = Sequential()\n",
        "\n",
        "# #Add two dense layers, one with 100 neurons and another with 10 for output. \n",
        "# #The first utilizes the relu activstion function and the other utilizes softmax activation function\n",
        "# model.add(Dense(units = 100, activation = 'relu'))\n",
        "# model.add(Dense(units = 10, activation = 'softmax'))\n",
        "\n",
        "# # Compile loss with Categorical Cross Entropy and utiize the Stochastic Gradient Descent with a learning rate of 0.05. \n",
        "# # We pull metrics for Accuracy \n",
        "# model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.SGD(lr=rate),metrics=['accuracy'])\n",
        "\n",
        "# # Initialize a dictionary which contains an array for the 4 main metrics\n",
        "# fit_history = {'acc':[],'loss':[],'val_acc':[],'val_loss':[]}\n",
        "\n",
        "# # Iterate through each epoch\n",
        "# for i in range(epochs):\n",
        "  \n",
        "#   # Show current epoch\n",
        "#   print('\\nActual Epoch: {}'.format(i+1))\n",
        "  \n",
        "#   # For each epoch train the model against X_train and Y_train and a given test data of X_test and Y_test\n",
        "#   hist = model.fit(X_train, Y_train, batch_size = batch_size, nb_epoch = 1, verbose = 2, validation_data = (X_test, Y_test))\n",
        "  \n",
        "#   # Push the 4 main updated metrics for each run to the fit history dictionaries. This will be used for building a metric plot.\n",
        "#   for each in fit_history: fit_history[each].append(hist.history[each])\n",
        "\n",
        "# # hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# # Print out the final metrics once the model is evaluated against the test data\n",
        "# for i in range(2): print('\\nFinal {} of test data: {}%'.format('Loss'if i==0 else 'Accuracy',model.evaluate(X_test, Y_test, verbose = 0)[i]*100))\n",
        "\n",
        "# # Plot the training metrics\n",
        "# for each in fit_history: k = plt.plot(fit_history[each], label=each)\n",
        "\n",
        "# # Show Plot with legend\n",
        "# plt.xlabel('Epochs'); plt.ylabel('Percent accuracy'); plt.legend()\n",
        "\n",
        "\n",
        "# preprocessing of x-training and x-testing data\n",
        "\n",
        "input(x_train.shape)\n",
        "x_train = (x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]))\n",
        "x_test = (x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
        "x_train = x_train.astype('float')\n",
        "x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "# concatenating x-training and x-testing features\n",
        "train_test = [np.zeros((x_train.shape[0],x_train.shape[1]+5)), np.zeros((x_test.shape[0],x_test.shape[1]+5))]\n",
        "\n",
        "for a,each in enumerate((x_train,x_test)):\n",
        "  for i in range(each.shape[0]):\n",
        "    for j in range(each.shape[1]):\n",
        "      train_test[a][i][j] = each[i][j]\n",
        "    train_test[a][i][784] = var_dict['con'][0][i]\n",
        "    train_test[a][i][785] = var_dict['x_width'][0][i]\n",
        "    train_test[a][i][786] = var_dict['x_height'][0][i]\n",
        "    train_test[a][i][787] = var_dict['x_bw'][0][0][i]\n",
        "    train_test[a][i][788] = var_dict['x_bw'][0][1][i]\n",
        "# for i in range(x_test.shape[0]):\n",
        "#   for j in range(x_test.shape[1]):\n",
        "#     test[i][j] = x_test[i][j]\n",
        "#   test[i][784] = var_dict['con'][1][i]\n",
        "#   test[i][785] = var_dict['x_width'][1][i]\n",
        "#   test[i][786] = var_dict['x_height'][1][i]\n",
        "#   test[i][787] = var_dict['x_bw'][1][0][i]\n",
        "#   test[i][788] = var_dict['x_bw'][1][1][i]\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, classes_size)\n",
        "y_test = keras.utils.to_categorical(y_test, classes_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}